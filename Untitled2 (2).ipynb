{"metadata":{"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy\nimport sys\nimport nltk\nnltk.download('stopwords')\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\nfrom keras.models imoprt Sequential\nfrom keras.layers import Dense,Dropout,LSTM\nfrom keras.utils import np_utils \nfrom keras.callbacks  import ModelCheckpoints\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file=open(\"frankenstein-2.txt\").read()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize_words(input):\n    input=input.lower()\n    tokenizer=RegexTokenizer(r'\\w+')\n    tokens=tokenizer.tokenize(input)\n    filtered =filter(lambda token: token not in stopwords.words('english'),tokens)\n    return \" \".join(filtered)\nprocessed_inputs=tokenize_words(file)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"char=sorted(list(set(processed_inputs)))\nchar_to_num=dict((c,i)for i,c i enumerate(chars))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_len=len(processed_inputs)\nvocab_len=len(chars)\nprint(\"total number of characters:\",input_len)\nprint(\"total vocab:\",vocab_len)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seq_length=100\nx_data=[]\ny_data=[]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(0,input_len-seg_length,1):\n    in_seq=processed_inputs[i:i+seq_length]\n    out_seq=processed_inputs[i+seq_length]\n    x_data.append([char_to_num[char] for char in in_seq])\n    y_data.append(char_to_num[out_seq])\nn_patterns=len(x_data)\nprint(\"total patterns:\",n_patterns)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X=numpy.reshape(x_data,(n_patterns,seq_length,1))\nX=X/float(vocab_len)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y=np.utils.to.categorical(y_data)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model Sequential()\nmodel.add (LSTM(256, input shape=(X.shape[1], X.shape[2]),return_sequences=True)) \nmodel.add ( Dropout (0.2))\nmodel.add (LSTM (256, return_sequences=True)) \nmodel.add (Dropout(0.2))\nmodel.add (LSTM(128))\nmodel.add (Dropout(0.2))\nmodel.add (Dense(y.shape[1], activation\"'softmax'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(loss=\"categorical_crossentropy\",optimizer='adam')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filepath=\"model_weights_saved.hdfs\"\ncheckpoint=ModelCheckpoint(filepath, monitor='loss'verbose=1,save_best_only=True,mode='min')\ndesired_callbacks=[checkpoint]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(X,y,epochs=4,batch_size=256,callbacks=desired_callbacks)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filename=\"model_weights_saved.hdf5\" \nmodel.load_weights(filenane) \nmodel.compile(loss='categorical_crossentropy',optimzer='adam')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nun_to_char.dict((i,c) for i,c in enumerate(chars))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start=numpy.random.randint(0,len(x_data) -1) \npattern=x_data[start]\nprint(\"Random Seed:\")\nprint(\"\\\"\".join(num_to_char[value]for value in pattern]),\"\\\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(1000):\n    x=numpy.reshape(pattern,(1,len(pattern), 1))\n    x=x/float(vocab_len)\n    prediction.model.predict(x, verbose=0)\n    index numpy.argmax(prediction) \n    result nun_to_char[index]\n    seq_in=[nun_to char[value] for value in pattern]\n    sys.stdout.write(result)\n    pattern.append(index) \n    pattern=pattern[1:len(pattern)]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"empted acconpany proceeded short distance house heart sensations folt see sooet ines would sensation searon \nsear persuade country persuade could sear feelings exes said i dear cons iderable shall see searon entered \nconsinuedforsed seened see season telpe aight i see searon entered consinued forned seened see seaon telpe aight \nsee searon entered cons inued formed seened see season telpn Mi ght see searon entered cons nued foried seened \nseo season 01p might see searon entered consinued forned sened see season telpe Might 8 oaron entered consinued","metadata":{},"execution_count":null,"outputs":[]}]}